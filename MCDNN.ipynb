{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Conv2D, concatenate, MaxPooling2D, Flatten, BatchNormalization, GlobalMaxPooling2D, Average\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# fix random seed for reproducibility\n",
    "# load data\n",
    "def LiSHT(x):\n",
    "  return x * K.tanh(x)\n",
    "\n",
    "def load_dataset():\n",
    "  (train_x,train_y), (test_x,test_y) =  mnist.load_data()\n",
    "  train_x = train_x.reshape((train_x.shape[0], 28, 28, 1))\n",
    "  test_x = test_x.reshape((test_x.shape[0], 28, 28, 1))\n",
    "  train_y = to_categorical(train_y)\n",
    "  test_y = to_categorical(test_y)\n",
    "  return train_x, train_y, test_x, test_y\n",
    "\n",
    "def data_prep(train,test):\n",
    "  # convert from integers to floats\n",
    "  train_norm = train.astype('float32')\n",
    "  test_norm = test.astype('float32')\n",
    "  # normalize to range 0-1\n",
    "  train_norm = train_norm / 255.0\n",
    "  test_norm = test_norm / 255.0\n",
    "  # return normalized images\n",
    "  return train_norm, test_norm\n",
    "\n",
    "def define_model(width):\n",
    "  input = Input(shape= (width,width,1)) \n",
    "\n",
    "  conv1_1 = Conv2D(20, kernel_size=(4,4), strides=(1,1), padding= 'same', activation= 'tanh', kernel_initializer= 'he_uniform')(input)\n",
    "  max1 = MaxPooling2D((2,2), strides = (2,2), padding = 'same')(conv1_1)\n",
    "  batch1 = BatchNormalization()(max1)\n",
    "  drop1 = Dropout(0.4)(batch1)\n",
    "  conv2 = Conv2D(40, kernel_size=(5,5),strides= (1,1),padding= 'same', activation= 'tanh', kernel_initializer= 'he_uniform')(drop1)\n",
    "  max2 = MaxPooling2D((3,3),strides = (3,3), padding = 'same')(conv2)\n",
    "  batch2 = BatchNormalization()(max2)\n",
    "  drop2 = Dropout(0.4)(batch2)\n",
    "  flat1= Flatten()(drop2)\n",
    "  dense1 = Dense(150, activation= 'tanh', kernel_initializer= 'he_uniform')(flat1)\n",
    "  output1 = Dense(10, activation= 'softmax')(dense1)\n",
    "  \n",
    "\n",
    "  #input = Input(shape= (width,width,1))\n",
    "  conv1_2 = Conv2D(20, kernel_size=(4,4), strides=(2,2), padding= 'same', activation= 'tanh', kernel_initializer= 'he_uniform')(input)\n",
    "  max1 = MaxPooling2D((2,2), strides = (2,2))(conv1_2)\n",
    "  dense2 = Dense(150, activation= 'tanh', kernel_initializer= 'he_uniform')(flat1)\n",
    "  output2 = Dense(10, activation= 'softmax')(dense2)\n",
    "\n",
    "  conv1_3 = Conv2D(20, kernel_size=(4,4), strides=(2,2), padding= 'same', activation= 'tanh', kernel_initializer= 'he_uniform')(input)\n",
    "  max1 = MaxPooling2D((2,2), strides = (2,2))(conv1_3)\n",
    "  dense3 = Dense(150, activation= 'tanh', kernel_initializer= 'he_uniform')(flat1)\n",
    "  output3 = Dense(10, activation= 'softmax')(dense3)\n",
    "\n",
    "\n",
    "  conv1_4 = Conv2D(20, kernel_size=(4,4), strides=(2,2), padding= 'same', activation= 'tanh', kernel_initializer= 'he_uniform')(input)\n",
    "  max1 = MaxPooling2D((2,2), strides = (2,2))(conv1_4)\n",
    "  dense4 = Dense(150, activation= 'tanh', kernel_initializer= 'he_uniform')(flat1)\n",
    "  output4 = Dense(10, activation= 'softmax')(dense4)\n",
    "\n",
    "\n",
    "  conv1_5 = Conv2D(20, kernel_size=(4,4), strides=(2,2), padding= 'same', activation= 'tanh', kernel_initializer= 'he_uniform')(input)\n",
    "  max1 = MaxPooling2D((2,2), strides = (2,2))(conv1_5)\n",
    "  dense5 = Dense(150, activation= 'tanh', kernel_initializer= 'he_uniform')(flat1)\n",
    "  output5 = Dense(10, activation= 'softmax')(dense5)\n",
    "\n",
    "\n",
    "  #concatenated = concatenate([output1,output2,output3,output4,output5])\n",
    "  ave = Average()([output1, output2, output3, output4, output5])\n",
    "  #output = Dense(10,activation= 'softmax')(ave)\n",
    "\n",
    "  #model = Model(inputs = [input_1,input_2,input_3,input_4,input_5], outputs = output )\n",
    "  model = Model(inputs = input, outputs = ave)\n",
    "\n",
    "  opt = SGD(lr = 0.0000015, momentum = 0.9)\n",
    "  model.compile(optimizer = opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def run_test_harness():\n",
    "  train_x, train_y, test_x,test_y = load_dataset()\n",
    "  train_x,test_x = data_prep(train_x, test_x)\n",
    "  #model = define_model(28)\n",
    "  datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, rotation_range =10, zoom_range = 0.1)\n",
    "  testgen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, rotation_range =10, zoom_range = 0.1)\n",
    "  it_train = datagen.flow(train_x, train_y, batch_size=64)\n",
    "  it_test = testgen.flow(test_x, test_y, batch_size=64)\n",
    "  steps = int(train_x.shape[0] / 64)\n",
    "  new_model.fit_generator(it_train, steps_per_epoch=steps, epochs=200, validation_data=it_test, verbose=2,shuffle = True)\n",
    "  new_model.save('final_model.h5')\n",
    "\n",
    "new_model = tf.keras.models.load_model('/content/drive/My Drive/Colab Notebooks/final_model (600).h5')\n",
    "\n",
    "run_test_harness()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
